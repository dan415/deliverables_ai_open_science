{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"deliverables_ai_open_science Description This Github repository serves the purpose of containing project deliveries for the subject of Artificial Intelligence and Open Science (Degree of Computer Engineering, Universidad Polit\u00e9cnica de Madrid). Each deliverable contains its own README.md with usage instructions and requirements Deliverable 1 https://deliverables-ai-open-science.readthedocs.io/en/latest/index.html Author Daniel Cabrera Rodr\u00edguez Github: @dan415 License Copyright (C) 1989, 1991 Free Software Foundation, Inc., Daniel Cabrera Rodr\u00edguez","title":"Home"},{"location":"index.html#deliverables_ai_open_science","text":"","title":"deliverables_ai_open_science"},{"location":"deliverable1/index.html","text":"deliverable1 Description First deliverable project for the Artificial Intelligence and Open Science course The purpose of this program is to convert and extract data from input PDF files in order to obtain the following: Draw a keyword cloud based on the words found in the abstract of your papers. Create a visualization showing the number of \ufb01gures per article. Create a list of the links found in each paper. Requirements wordcloud==1.8.2.2 matplotlib==3.4.3 grobid-client==0.8.3 nltk==3.8.1 Running on Docker version 20.10.17, build 100c70180f conda 4.12.0 Ubuntu 22.04 Install From source git clone https://github.com/dan415/deliverables_ai_open_science.git cd deliverable1 conda env create -f environment.yml conda activate openscience docker pull lfoppiano/grobid:0.7.2 Using docker docker pull lfoppiano/grobid:0.7.2 docker pull danicabrera/delivery1:latest Usage Move files to be analyzed to the input_files folder. If using docker: ./input_files and ./output_files are expected to be used as the folders for the input and output files, respectively. From source docker run -t --rm -p 8070:8070 lfoppiano/grobid:0.7.2 & python src/pdf_analysis.py Alternatively, you can specify the folder for the input pdf files and the desired output folder: python src/pdf_analysis.py --INPUT_PATH [input path] --OUTPUT_PATH [output path] You can also set the configuration file for the grobid client, needed to process the documents: python src/pdf_analysis.py --INPUT_PATH [input path] --OUTPUT_PATH [output path] --GROBID_CLIENT_CONFIG_PATH [grobid config path] None of them are required arguments. By default, these are set as ./input_files, ./output_files, ./config.json, respectively Using Docker Note: if you want to build the docker image from source, uncomment on the docker-compose file this line # build: . # if you want to build the image locally (line 13) and comment this one: image: danicabrera/delivery1:latest (line 14) Then: cd deliverable1 sudo docker-compose up Author Daniel Cabrera Rodr\u00edguez Github: @dan415 License Copyright (C) 1989, 1991 Free Software Foundation, Inc., Daniel Cabrera Rodr\u00edguez","title":"Installation"},{"location":"deliverable1/index.html#deliverable1","text":"","title":"deliverable1"},{"location":"deliverable1/rationale.html","text":"Assumptions Grobid must be running at the time of execution Grobid's config file provided to the program must exist, when running from source provided input_files folder must exist, when running from source provided output_files folder must exist, when running from source Algorithm This program will try to process all PDF files stored in given input files folder and produce: - A grobid-generated TEI xml for each of the files - A links.txt file in json format with a list of encountered links for each of the files - wordcloud.png A wordcloud image, generated with the concatenated text of all abstracts in the files - figures.png A bar plot summarizing the count of figures found in each file Usage usage: PDF Analysis [-h] [--INPUT_PATH INPUT_PATH] [--OUTPUT_PATH OUTPUT_PATH] [--GROBID_CLIENT_CONFIG_PATH GROBID_CLIENT_CONFIG_PATH] Analyse PDFs using GROBID options: -h, --help show this help message and exit --INPUT_PATH INPUT_PATH Path to the input directory containing the PDFs to be analysed --OUTPUT_PATH OUTPUT_PATH --OUTPUT_PATH OUTPUT_PATH Path to the output directory where the results will be stored --GROBID_CLIENT_CONFIG_PATH GROBID_CLIENT_CONFIG_PATH Path to the GROBID client configuration file Objectives of this program Draw a keyword cloud based on the abstract information Create a visualization showing the number of figures per article. Create a list of the links found in each paper. All outputs are saved in defined output folder Objective 1 For objective 1, library wordcloud is used. For more precise results, stopwords are identified and eliminated from the text. The langauge for the set of stop words to use are obtained from the lang attribute from the TEI header . Stop word removal will not be used if unable to identify said attribute or language code is not one of acccepted langauges. Accepted langauges are: - spanish - english - german - french - portuguese - italian The wordcloud is stored as a PNG file. Objective 2 For objective 2, the way figures are identified is by searching for the tag figure in the xml tree structure. The counts are obtained and then plotted with matplotlib Objective 3 For objective 3, links are identified in two ways. For DOI's present in the file, the generated ptr tags are used to retrieve the links. Next, Using the text of the PDF file, a regex expression is used to find link-formatted text.","title":"Rationale"},{"location":"deliverable1/test/tests.html","text":"Tests The approach is to execute the program changing the input files folder to test for different types of input. Metrics can be obtained from the summary.json file to check if the program is working correctly. Preconditions: Grobid must be running, dependencies must be installed and environment must be activated. Details on how to do this can be found in the README.md file of deliverable1. Run on folder deliverables_ai_open_science: python -m unittest deliverable1.test.test_analyzer test_01_blank_pdf case_01 Run with blank pdf file. Should output empty \"figures.png\", \"links.txt\" and \"summary.json\", but no wordcloud. Should not produce any exceptions. If run with docker-compose, tei file will be generated, else it will not. test_02_correct_num_papers case_02 Run with 3 valid pdfs. Should output 7 files, and produce no exceptions test_03_correct_num_figures case_03 Run with 4 valid pdfs, total number of figures in summary.json should be 25. Should not produce any exceptions. test_03_correct_num_links case_03 Run with 4 valid pdfs, total number of links in summary.json should be 8. Should not produce any exceptions.","title":"Tests"}]}