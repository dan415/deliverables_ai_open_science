{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"deliverables_ai_open_science Description This Github repository serves the purpose of containing project deliveries for the subject of Artificial Intelligence and Open Science (Degree of Computer Engineering, Universidad Polit\u00e9cnica de Madrid). Each deliverable contains its own README.md with usage instructions and requirements Author Daniel Cabrera Rodr\u00edguez Github: @dan415 License Copyright (C) 1989, 1991 Free Software Foundation, Inc., Daniel Cabrera Rodr\u00edguez","title":"Home"},{"location":"index.html#deliverables_ai_open_science","text":"","title":"deliverables_ai_open_science"},{"location":"deliverable1/index.html","text":"deliverable1 Description First deliverable project for the Artificial Intelligence and Open Science course The purpose of this program is to convert and extract data from input PDF files in order to obtain the following: Draw a keyword cloud based on the words found in the abstract of your papers. Create a visualization showing the number of \ufb01gures per article. Create a list of the links found in each paper. Requirements wordcloud==1.8.2.2 matplotlib==3.4.3 grobid-client==0.8.3 nltk==3.8.1 Running on Docker version 20.10.17, build 100c70180f conda 4.12.0 Ubuntu 22.04 Install From source git clone https://github.com/dan415/deliverables_ai_open_science.git cd deliverable1 conda env create -f environment.yml conda activate openscience docker pull lfoppiano/grobid:0.7.2 Using docker docker pull lfoppiano/grobid:0.7.2 docker pull lfoppiano/grobid:0.7.2 Usage From source docker run -t --rm -p 8070:8070 lfoppiano/grobid:0.7.2 & python src/pdf_analysis.py Alternatively, you can specify the folder for the input pdf files and the desired output folder: python src/pdf_analysis.py --INPUT_PATH [input path] --OUTPUT_PATH [output path] You can also set the configuration file for the grobid client, needed to process the documents: python src/pdf_analysis.py --INPUT_PATH [input path] --OUTPUT_PATH [output path] --GROBID_CLIENT_CONFIG_PATH [grobid config path] None of them are required arguments. By default, these are set as ./input_files, ./output_files, ./config.json, respectively Using Docker cd deliverable1 sudo docker-compose up Author Daniel Cabrera Rodr\u00edguez Github: @dan415 License Copyright (C) 1989, 1991 Free Software Foundation, Inc., Daniel Cabrera Rodr\u00edguez","title":"Installation"},{"location":"deliverable1/index.html#deliverable1","text":"","title":"deliverable1"},{"location":"deliverable1/rationale.html","text":"Assumptions Grobid must be running at the time of execution Grobid's config file provided to the program must exist, when running from source provided input_files folder must exist, when running from source provided output_files folder must exist, when running from source Algorithm This program will try to process all PDF files stored in given input files folder and produce: - A grobid-generated TEI xml for each of the files - A links.txt file in json format with a list of encountered links for each of the files - wordcloud.png A wordcloud image, generated with the concatenated text of all abstracts in the files - figures.png A bar plot summarizing the count of figures found in each file Usage usage: PDF Analysis [-h] [--INPUT_PATH INPUT_PATH] [--OUTPUT_PATH OUTPUT_PATH] [--GROBID_CLIENT_CONFIG_PATH GROBID_CLIENT_CONFIG_PATH] Analyse PDFs using GROBID options: -h, --help show this help message and exit --INPUT_PATH INPUT_PATH Path to the input directory containing the PDFs to be analysed --OUTPUT_PATH OUTPUT_PATH --OUTPUT_PATH OUTPUT_PATH Path to the output directory where the results will be stored --GROBID_CLIENT_CONFIG_PATH GROBID_CLIENT_CONFIG_PATH Path to the GROBID client configuration file Objectives of this program Draw a keyword cloud based on the abstract information Create a visualization showing the number of figures per article. Create a list of the links found in each paper. All outputs are saved in defined output folder Objective 1 For objective 1, library wordcloud is used. For more precise results, stopwords are identified and eliminated from the text. The langauge for the set of stop words to use are obtained from the lang attribute from the TEI header . Stop word removal will not be used if unable to identify said attribute or language code is not one of acccepted langauges. Accepted langauges are: - spanish - english - german - french - portuguese - italian The wordcloud is stored as a PNG file. Objective 2 For objective 2, the way figures are identified is by searching for the tag figure in the xml tree structure. The counts are obtained and then plotted with matplotlib Objective 3 For objective 3, links are identified in two ways. For DOI's present in the file, the generated ptr tags are used to retrieve the links. Next, Using the text of the PDF file, a regex expression is used to find link-formatted text.","title":"Rationale"},{"location":"deliverable1/test/tests.html","text":"","title":"Tests"}]}